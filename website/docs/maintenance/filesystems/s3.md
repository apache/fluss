---
title: Amazon S3
sidebar_position: 4
---

<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-->

# Amazon S3

[Amazon Simple Storage Service](http://aws.amazon.com/s3/) (Amazon S3) is a cloud object storage with industry-leading scalability, data availability, security, and performance.

## Configurations setup

To enable S3 as remote storage, there are some additional configurations that must be set in Fluss' `server.yaml` and, depending on the authentication method, also in the Flink configuration. 

Fluss and its Flink Connector rely internally on [Hadoop AWS](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/).
A full list of configuration options for Hadoop AWS can be found [here](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/index.html#General_S3A_Client_configuration). 
If you follow the procedure below, they are automatically passed to Hadoop AWS.

For configuring **Fluss CoordinatorServers and TabletServers**, put the configuration options in `server.yaml`. 
We recommend to use the prefix `s3.`, e.g., `s3.access-key`.
However, the following prefixes are also supported: `s3a.` `fs.s3a.`.

For configuring the **Fluss Flink Connector**, put the configuration options in the Flink configuration of your **JobManager**.
We recommend to use the prefix `fluss.client.s3`, e.g., `fluss.client.s3.access-key`.
However, the following prefixes are also supported: `fluss.client.s3a.` `fluss.client.fs.s3a.`.

Additionally, there are also configuration options that are _not_ passed to the Hadoop AWS library and are internal to Fluss, e.g., those with prefix `fs.s3`/`fluss.client.fs.s3`.

### Use Fluss Credential Provider with Fluss-Internal Token Delegation Process

**When to use?**   
- When at least one client is not running on AWS **and**
- you are using AWS S3 (**not** an S3-compatible object storage).

:::info
This method can be used when all clients are running on AWS as well, but you should prefer [predefined credential providers that use temporary credentials](#predefined-credential-providers-that-use-temporary-credentials), because this avoids embedding long-term access keys and secrets on the server.
:::

The token delegation process avoids embedding long-term credentials on the _client side_. 
Instead, Fluss clients continuously retrieve temporary credentials from the servers. 
The servers create and automatically refresh these temporary credentials via Security Token Service (STS). 
Fluss currently supports the following STS APIs: [`GetSessionToken`](https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html) 

**Fluss CoordinatorServer/TabletServer**

```yaml
remote.data.dir: s3://<your-bucket>/path/to/remote/storage
s3.access-key: <your-access-key>
s3.secret-key: <your-secret-key>
s3.region: <your-s3-region>
```

**Flink JobManager/TaskManager**

All necessary information for the Flink client will be distributed with the token delegation process.

### Use Another Credential Provider

#### Predefined Credential Providers That Use Temporary Credentials

**When to use?**   
- When all clients are running on AWS **and**
- you are using AWS S3 (**not** an S3-compatible object storage).

When all clients are running on AWS you can rely on predefined AWS credential providers that use temporary credentials and also refresh them automatically.

First, deactivate the token delegation process.

**Fluss CoordinatorServer/TabletServer**

```yaml
fs.s3.enable-token-delegation: false
```

Then configure all Fluss CoordinatorServers/TabletServers and Fluss Flink Connectors that run on AWS to use a predefined credential provider that uses temporary credentials. 
Details on the configuration are beyond scope and highly dependent on your setup. 
Please refer to the [respective documentation](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/index.html#auth_providers). 
Check the documentation carefully which credential providers use _temporary credentials_!


#### Predefined Credential Providers That Use Long-Term Credentials

**When to use?**   
When you are using an AWS S3-compatible object storage.

:::warning
We only recommend to use this method if

- you need to access an S3-**compatible** object storage **and**
- all clients are running in a trusted environment, e.g., a Kubernetes cluster with controlled access, as this method requires embedding long-term credentials in clients.

For S3-compatible object storages that support STS (e.g., [MinIO](https://min.io/)), it is in theory possible to use the Fluss credential provider with the delegation token process (see [this issue](https://github.com/alibaba/fluss/issues/621)).
However, there are also S3-compatible object storages that currently do not support STS at all, e.g., [Hetzner Cloud Object Storage](https://docs.hetzner.com/storage/object-storage/overview/). 

**Use at your own risk!**
:::

This authentication method requires **Flink 1.19 or higher**.

**Fluss CoordinatorServer/TabletServer**

```yaml
remote.data.dir: s3://<your-bucket>/path/to/remote/storage
fs.s3.enable-token-delegation: false
s3.access-key: <your-access-key, e.g., MinIO username>
s3.secret-key: <your-secret-key, e.g., MinIO password>
# Set to true depending on S3-compatible object storage. E.g., for MinIO set to true.
s3.path.style.access: true
s3.endpoint: <your endpoint>
# May be empty not necessary to set for some S3-compatible object storages, e.g., for MinIO. Check the corresponding documentation.
s3.region: <your-region>
```

**Flink JobManager**

```yaml
fluss.client.fs.s3.access-key: <your-access-key, e.g., MinIO username>
fluss.client.fs.s3.secret-key:  <your-secret-key, e.g., MinIO password>
# Set to true depending on S3-compatible object storage. E.g., for MinIO set to true.
fluss.client.fs.s3.path.style.access: true
fluss.client.fs.s3.endpoint: <your endpoint>
# May be empty not necessary to set for some S3-compatible object storages, e.g., for MinIO. Check the corresponding documentation.
fluss.client.fs.s3.region: <your-region>
```