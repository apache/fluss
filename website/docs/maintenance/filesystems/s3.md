---
title: Amazon S3
sidebar_position: 4
---

# Amazon S3

[Amazon Simple Storage Service](http://aws.amazon.com/s3/) (Amazon S3) is a cloud object storage with industry-leading scalability, data availability, security, and performance.

## Configurations setup

To enable S3 as remote storage, there are some additional configurations that must be set in Fluss and, depending on the authentication method (credential provider), also in the Flink JobManager configuration. 

For S3 support, Fluss and its Flink Connector internally rely on [Hadoop AWS](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/).
A full list of configuration options for Hadoop AWS can be found [here](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/index.html#General_S3A_Client_configuration). 

For configuring **Fluss CoordinatorServers and TabletServers**, put the configuration options in `server.yaml`. 
We recommend to use the prefix `s3.`, e.g., `s3.access-key`.
However, the following prefixes are also supported: `s3a.` `fs.s3a.`.

For configuring the **Fluss Flink Connector**, put the configuration options in the Flink configuration of your **JobManager**.
We recommend to use the prefix `fluss.client.s3`, e.g., `fluss.client.s3.access-key`.
However, the following prefixes are also supported: `fluss.client.s3a.` `fluss.client.fs.s3a.`.

Additionally, there are also configuration options that are _not_ passed to the Hadoop AWS library and are internal to Fluss. Those have prefix `fs.s3`/`fluss.client.fs.s3`.

### Use Fluss Credential Provider with Fluss-Internal Token Delegation Process

**When to use?**   
- When at least one Fluss component (client, server) is not running on AWS **and**
- you are using AWS S3 (**not** an S3-compatible object storage).

:::warning
This method in general also works when all Fluss components are running on AWS. However, you should prefer [Temporary AWS Credential Providers](#temporary-aws-credential-providers), as they also avoid embedding long-term access keys and secrets on the server.
:::

The token delegation process avoids embedding long-term credentials on the _client side_. 
Fluss clients continuously retrieve temporary credentials from the servers (token delegation). 
The servers create and automatically refresh these temporary credentials via Security Token Service (STS). 

Fluss currently supports the following STS APIs: [`GetSessionToken`](https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html) 

**Fluss CoordinatorServer/TabletServer**

```yaml
remote.data.dir: s3://<your-bucket>/path/to/remote/storage
s3.endpoint: <your endpoint>
s3.access-key: <your-access-key>
s3.secret-key: <your-secret-key>
s3.region: <your-s3-region>
```

**Flink JobManager/TaskManager**

All necessary information for the Fluss Flink client will be provided by the server.

### Use Another Credential Provider

:::note
If no credential provider is explicitly set (`s3.aws.credentials.provider`), Fluss uses the default provider chain of Hadoop AWS.
:::

If you are not using the Fluss-internal token delegation process, each Fluss component authenticates itself with its own set of credentials.

#### Temporary AWS Credential Providers

**When to use?**   
- When all clients are running on AWS **and**
- you are using AWS S3 (**not** an S3-compatible object storage).

When all Fluss components are running on AWS and you are using AWS S3, we recommend to use temporary AWS credential providers.

:::warning
Carefully check which credential providers use [_temporary credentials_](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/index.html#auth_providers) (e.g., `org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider`).
:::

**Fluss CoordinatorServer/TabletServer**

```yaml
remote.data.dir: s3://<your-bucket>/path/to/remote/storage
fs.s3.enable-token-delegation: false
s3.endpoint: <your endpoint>
s3.region: <your-s3-region>
#  Use if the desired provider is not in the default provider chain
s3.aws.credentials.provider: <AWS credential provider(s)>
```

**Flink JobManager**

**Note:** `s3.path-style-access`, `s3.endpoint`, `s3.region` are provided by the Fluss server.

```yaml
# Use if the desired provider is not in the default provider chain
fluss.client.fs.s3.aws.credentials.provider: <AWS credential provider(s)>
```

For further details, please refer to the [respective documentation](https://hadoop.apache.org/docs/r3.3.4/hadoop-aws/tools/hadoop-aws/index.html#auth_providers). 


#### Credential Providers That Use Long-Term Credentials

**When to use?**   
When you are using an AWS S3-compatible object storage.

:::warning
We only recommend to use this method if

- you need to access an S3-**compatible** object storage **and**
- all clients are running in a trusted environment, e.g., a Kubernetes cluster with controlled access, as this method requires embedding long-term credentials in clients.

For S3-compatible object storage that support STS (e.g., [MinIO](https://min.io/)), we are working towards supporting Fluss's token delegation process (see [this issue](https://github.com/alibaba/fluss/issues/621)).
However, there are also S3-compatible object storages that currently do not support STS at all, e.g., [Hetzner Cloud Object Storage](https://docs.hetzner.com/storage/object-storage/overview/). 

**Use at your own risk!**
:::

This authentication method requires **Flink 1.19 or higher**.

**Fluss CoordinatorServer/TabletServer**

```yaml
remote.data.dir: s3://<your-bucket>/path/to/remote/storage
fs.s3.enable-token-delegation: false
s3.endpoint: <your endpoint>
s3.access-key: <your-access-key, e.g., MinIO username>
s3.secret-key: <your-secret-key, e.g., MinIO password>
# Set to true depending on the S3-compatible object storage. 
# For MinIO set to true.
s3.path-style-access: true
# May be empty depending on the S3-compatible object storage. Check the corresponding documentation.
# For MinIO, you don't need to set a region.
s3.region: <your-region>
```

**Flink JobManager**

**Note:** `s3.path-style-access`, `s3.endpoint`, `s3.region` are provided by the Fluss server.

```yaml
fluss.client.fs.s3.access-key: <your-access-key, e.g., MinIO username>
fluss.client.fs.s3.secret-key:  <your-secret-key, e.g., MinIO password>
```